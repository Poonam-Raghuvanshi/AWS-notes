<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title> S3 </title>
    <link rel="stylesheet" href="../style.css">
</head>

<body>
    <p class="main-heading">Amazon S3</p>

    <p>
        You can use Amazon S3 to host a static website. On a static website, individual web pages include static
        content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an
        Amazon S3 bucket for website hosting and then upload your website content to the bucket. <br> When you configure
        a bucket as a static website, you must enable website hosting, set permissions, and create and add an index
        document. Depending on your website requirements, you can also configure redirects, web traffic logging, and a
        custom error document.

        <b> Distributing the static content through S3 allows us to offload most of the network usage to S3 and free up
            our applications running on ECS.</b>
    </p>
    <p class="heading">S3 Bucket Policies</p>
    <p>Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects within a
        single bucket. Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management
        of permissions. With bucket policies, you can grant users within your AWS Account or other AWS Accounts access
        to your Amazon S3 resources.
    </p>
    <p>
        <b>You can further restrict access to specific resources based on certain conditions. For example, you can
            restrict access based on request time (Date Condition), whether the request was sent using SSL (Boolean
            Conditions), a requester’s IP address (IP Address Condition), or based on the requester's client application
            (String Conditions). To identify these conditions, you use policy keys.</b>
    </p>
    <p class="heading">S3 Versioning</p>
    <p> Versioning is a means of keeping multiple variants of an object in the same bucket. You can use versioning to
        preserve, retrieve, and restore every version of every object stored in your Amazon S3 bucket.
        Versioning-enabled buckets enable you to recover objects from accidental deletion or overwrite.</p>
    <p class="heading">Enable MFA delete on the bucket</p>
    <p>To provide additional protection, multi-factor authentication (MFA) delete can
        be enabled. MFA delete requires secondary authentication to take place before objects can be permanently deleted
        from an Amazon S3 bucket. </p>

    <img src="../images/s3.PNG" />
    <p class="heading"> S3 storage classes</p>
    <p>S3 Storage Classes can be configured at the object level, and a single bucket can contain objects
        stored across S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. </p>
    <p class="point"> S3 Standard</p>
    <p> Store data in a minimum of three Availability Zones (AZs)</p>
    <p class="point">S3 Standard-IA</p>
    <p></p>
    <p class="point"> S3 One Zone-IA</p>
    <p>S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 One
        Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is
        ideal for customers who want a lower-cost option for infrequently accessed and re-creatable data but do not
        require the availability and resilience of S3 Standard or S3 Standard-I. S3 One Zone-IA offers the same high
        durability, high throughput, and low latency of S3 Standard, with a low per GB storage price and per GB
        retrieval fee.</p>
    <p class="point"> S3 Intelligent-Tiering</p>
    <p></p>
    <p class="point"> S3 Glacier</p>
    <p></p>
    <p class="point"> S3 Glacier Deep Archive</p>
    <p></p>

    <p class="heading">Lifecycle policy</p>
    <p> Automatically transition objects between storage classes without any application changes: <b> The minimum
            storage duration is 30 days before you can transition objects from S3 Standard to S3 One Zone-IA.</b>

    </p>
    <img src="../images/s31.jpg" />

    <p class="heading"> S3 Read-after-write consistency</p>
    <p>Delivers strong read-after-write consistency automatically, without changes to performance or
        availability, without sacrificing regional isolation for applications, and at no additional cost. <b> After a
            successful write of a new object or an overwrite of an existing object, any subsequent read request
            immediately receives the latest version of the object</b>. S3 also provides strong consistency for list
        operations, so after a write, you can immediately perform a listing of the objects in a bucket with any changes
        reflected. Strong read-after-write consistency helps when you need to immediately read an object after a write.

        To summarize, <b> all S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or
            metadata, are strongly consistent</b>. What you write is what you will read, and the results of a LIST will
        be an accurate reflection of what’s in the bucket.
    </p>
    <p class="heading"> S3 Copy Data</p>
    <p>Copy data from the source bucket to the destination bucket using the aws S3 sync command

        The aws S3 sync command uses the CopyObject APIs to copy objects between S3 buckets. The sync command lists the
        source and target buckets to identify objects that are in the source bucket but that aren't in the target
        bucket. The command also identifies objects in the source bucket that have different LastModified dates than the
        objects that are in the target bucket. The sync command on a versioned bucket copies only the current version of
        the object—previous versions aren't copied. By default, this preserves object metadata, but the access control
        lists (ACLs) are set to FULL_CONTROL for your AWS account, which removes any additional ACLs. If the operation
        fails, you can run the sync command again without duplicating previously copied objects.

        You can use the command like so:

        aws s3 sync s3://DOC-EXAMPLE-BUCKET-SOURCE s3://DOC-EXAMPLE-BUCKET-TARGET

        Set up S3 batch replication to copy objects across S3 buckets in different Regions using S3 console

        S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was
        in place, objects that have previously been replicated, and objects that have failed replication. This is done
        through the use of a Batch Operations job.</p>
    <p>

        You should note that batch replication differs from live replication which continuously and automatically
        replicates new objects across Amazon S3 buckets. You cannot directly use the AWS S3 console to configure
        cross-Region replication for existing objects. By default, replication only supports copying new Amazon S3
        objects after it is enabled using the AWS S3 console. Replication enables automatic, asynchronous copying of
        objects across Amazon S3 buckets. Buckets that are configured for object replication can be owned by the same
        AWS account or by different accounts. Object may be replicated to a single destination bucket or multiple
        destination buckets. Destination buckets can be in different AWS Regions or within the same Region as the source
        bucket.

        If you want to enable live replication for existing objects for your bucket, you must contact AWS Support and
        raise a support ticket. This is required to ensure that replication is configured correctly.</p>
    <p class="heading"> S3 Transfer Acceleration</p>
    <p> Bucket-level feature that <b>enables fast, easy, and secure transfers of files over
            long distances between your client and an S3 bucket</b>.</p>
</body>

</html>